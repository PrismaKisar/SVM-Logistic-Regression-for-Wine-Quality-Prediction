{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbf384cf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this notebook, optimal hyperparameters will be selected and the performance of both models will be evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd80834",
   "metadata": {},
   "source": [
    "### Imports\n",
    "The analysis commences with the necessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dbccc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd()\n",
    "while not (project_root / \"src\").exists():\n",
    "    project_root = project_root.parent\n",
    "\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "from model_selection import grid_search_cv\n",
    "from models import SVM, LogisticRegression\n",
    "from util import calculate_accuracy, calculate_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebe8b37",
   "metadata": {},
   "source": [
    "### Notebook Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d829b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "METRIC = 'accuracy'\n",
    "CV = 5\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d7df4",
   "metadata": {},
   "source": [
    "### Data Loading\n",
    "The data will be loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac528db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.read_csv('../data/processed/X_train.csv')\n",
    "y_train_df = pd.read_csv('../data/processed/y_train.csv')\n",
    "X_test_df = pd.read_csv('../data/processed/X_test.csv')\n",
    "y_test_df = pd.read_csv('../data/processed/y_test.csv')\n",
    "\n",
    "y_train = np.where(y_train_df['quality'] >= 6, 1, -1)\n",
    "y_test = np.where(y_test_df['quality'] >= 6, 1, -1)\n",
    "\n",
    "X_train = X_train_df.to_numpy()\n",
    "X_test = X_test_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb260a86",
   "metadata": {},
   "source": [
    "# Linear Models Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c97e23",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9635426a",
   "metadata": {},
   "source": [
    "### SVM\n",
    "For SVMs, two primary parameters require optimization: the number of iterations (*n_iters*) and the regularization parameter lambda (*lambda_param*). Typically, the number of folds ranges between 5 to 10. According to SVM theory, the higher the number of iterations, the lower the error should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7e511",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_n_iters_list = [100, 200, 500, 1000, 2000, 5000]\n",
    "svm_lambda_param_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "svm_param_grid = {\n",
    "        'n_iters': svm_n_iters_list,\n",
    "        'lambda_param' : svm_lambda_param_list\n",
    "    }\n",
    "\n",
    "svm_best_params, svm_best_metrics = grid_search_cv(SVM, svm_param_grid, X_train, y_train, cv=CV, scoring=METRIC, random_state=RANDOM_STATE)\n",
    "print(f'SVM best parameter: {svm_best_params}')\n",
    "print(f'SVM best metrics: {svm_best_metrics}')\n",
    "\n",
    "svm_n_iters = svm_best_params['n_iters']\n",
    "svm_lambda_param = svm_best_params['lambda_param']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1bf5da",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "As with SVMs, the parameters include *n_iters* and *lambda_param*, however, this model additionally incorporates the learning rate parameter (*learning_rate*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60341c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_n_iters_list = [2, 5, 10, 20, 50]\n",
    "lr_lambda_param_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "lr_learning_rate_list = [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "\n",
    "lr_param_grid = {\n",
    "        'n_iters': lr_n_iters_list,\n",
    "        'lambda_param': lr_lambda_param_list,\n",
    "        'learning_rate': lr_learning_rate_list\n",
    "    }\n",
    "\n",
    "lr_best_params, lr_best_metrics = grid_search_cv(LogisticRegression, lr_param_grid, X_train, y_train, cv=CV, scoring=METRIC, random_state=RANDOM_STATE)\n",
    "print(f'Logistic Regression best parameter: {lr_best_params}')\n",
    "print(f'Logistic Regression best metrics: {lr_best_metrics}')\n",
    "\n",
    "lr_n_iters = lr_best_params['n_iters']\n",
    "lr_lambda_param = lr_best_params['lambda_param']\n",
    "lr_learning_rate = lr_best_params['learning_rate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a04598",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "It is particularly valuable to analyze the learning curves of the various algorithms to observe how and when convergence occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bdba25",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be13d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(model_class, X_train, y_train, X_test, y_test, iterations_list, figname=None, **model_kwargs):\n",
    "    \n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    \n",
    "    for n_iter in iterations_list:\n",
    "        model = model_class(n_iters=n_iter, **model_kwargs)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        \n",
    "        train_score = calculate_accuracy(train_pred, y_train)\n",
    "        test_score = calculate_accuracy(test_pred, y_test)\n",
    "        \n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        \n",
    "        print(f\"Iter {n_iter}: Train={train_score:.3f}, Test={test_score:.3f}\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(iterations_list, train_scores, 'o-', label='Training', color='blue')\n",
    "    plt.plot(iterations_list, test_scores, 'o-', label='Test', color='red')\n",
    "    \n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "    if figname is not None:\n",
    "        plt.savefig(f'../plots/{figname}.pdf')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964f2023",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5f0c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(SVM, X_train, y_train, X_test, y_test, svm_n_iters_list, 'SVM learning curve', lambda_param=svm_lambda_param, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80621632",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d615089",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(LogisticRegression, X_train, y_train, X_test, y_test, lr_n_iters_list, 'LR learning curve', lambda_param=lr_lambda_param, learning_rate=lr_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b32c089",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "It is evident that SVM requires significantly more iterations than logistic regression, which is expected given that logistic regression updates parameters for each example at every iteration. Additionally, SVM performance has improved, while logistic regression exhibits slight overfitting tendencies as iterations progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d9c14e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bf0998",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7e192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_for_print = []\n",
    "def plot_metrics(predictions, y_test, figname=None):\n",
    "    metrics = calculate_metrics(predictions, y_test)\n",
    "    metrics_for_print.append(metrics)\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    names = ['Accuracy', 'Precision', 'Recall', 'F1']\n",
    "    values = [metrics['accuracy'], metrics['precision'], metrics['recall'], metrics['f1']]\n",
    "    \n",
    "    ax1.bar(names, values, color=['skyblue', 'lightcoral', 'lightgreen', 'orange'])\n",
    "    ax1.set_ylim(0, 1)\n",
    "    ax1.set_title('Metrics')\n",
    "    \n",
    "    for i, v in enumerate(values):\n",
    "        ax1.text(i, v + 0.02, f'{v:.3f}', ha='center')\n",
    "    \n",
    "    cm = np.array([[metrics['tn'], metrics['fp']], [metrics['fn'], metrics['tp']]], dtype=float)\n",
    "    \n",
    "    cm_normalized = cm / cm.sum()\n",
    "    \n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues', ax=ax2, xticklabels=['Bad', 'Good'], yticklabels=['Bad', 'Good'])\n",
    "    \n",
    "    ax2.set_title('Confusion Matrix (%)')\n",
    "    ax2.set_xlabel('Predicted')\n",
    "    ax2.set_ylabel('Actual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    if figname is not None:\n",
    "        plt.savefig(f'../plots/{figname}.pdf')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6a7a52",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b567659",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM(svm_n_iters, svm_lambda_param)\n",
    "svm.fit(X_train, y_train)\n",
    "predictions = svm.predict(X_test)\n",
    "plot_metrics(predictions, y_test, 'SVM stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973b2a4e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89dc801",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(lr_n_iters, lr_lambda_param, lr_learning_rate)\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "plot_metrics(predictions, y_test, 'LR stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13dd607",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Performance visualization indicates that logistic regression generally demonstrates superior performance on this specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d648ed3",
   "metadata": {},
   "source": [
    "# Kernel Models Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e1a829",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45af1341",
   "metadata": {},
   "source": [
    "### SVM\n",
    "Non-linear kernel models will now be tuned with the previous best parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cf532",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_AVERAGED_STATES = 10\n",
    "svm_k_param_grid = {\n",
    "        'kernel': ['poly'],\n",
    "        'n_iters': svm_n_iters_list,\n",
    "        'lambda_param': svm_lambda_param_list,\n",
    "        'degree': [2],\n",
    "        'n_averaged_states': [N_AVERAGED_STATES]\n",
    "    }\n",
    "\n",
    "svm_k_best_params, svm_k_best_metrics = grid_search_cv(SVM, svm_k_param_grid, X_train, y_train, cv=CV, scoring=METRIC, random_state=RANDOM_STATE)\n",
    "print(f'SVM best parameter: {svm_k_best_params}')\n",
    "print(f'SVM best metrics: {svm_k_best_metrics}')\n",
    "\n",
    "svm_degree = svm_k_best_params['degree']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2141ab",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038bc55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_k_param_grid = {\n",
    "        'kernel': ['poly'],\n",
    "        'n_iters': lr_n_iters_list,\n",
    "        'lambda_param': lr_lambda_param_list,\n",
    "        'learning_rate': lr_learning_rate_list,\n",
    "        'degree': [2]\n",
    "    }\n",
    "\n",
    "lr_k_best_params, lr_k_best_metrics = grid_search_cv(LogisticRegression, lr_k_param_grid, X_train, y_train, cv=CV, scoring=METRIC, random_state=RANDOM_STATE)\n",
    "print(f'SVM best parameter: {lr_k_best_params}')\n",
    "print(f'SVM best metrics: {lr_k_best_metrics}')\n",
    "\n",
    "lr_degree = lr_k_best_params['degree']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802d61b7",
   "metadata": {},
   "source": [
    "## Learning Curves\n",
    "It is particularly valuable to analyze the learning curves of the various algorithms to observe how and when convergence occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b9244a",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a34c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(SVM, X_train, y_train, X_test, y_test, svm_n_iters_list, 'k-SVM learning curve', lambda_param=svm_lambda_param, kernel='poly', degree=svm_degree, n_averaged_states=N_AVERAGED_STATES, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bcfc49",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016b60bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(LogisticRegression, X_train, y_train, X_test, y_test, lr_n_iters_list, 'k-LR learning curve', lambda_param=lr_lambda_param, learning_rate=lr_learning_rate, kernel='poly', degree=lr_degree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1d3437",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "It is evident that SVM requires significantly more iterations than logistic regression, which is expected given that logistic regression updates parameters for each example at every iteration. Additionally, SVM performance has improved, while logistic regression exhibits slight overfitting tendencies as iterations progress."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3811617",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bc56a1",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11714a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVM(n_iters=svm_n_iters, lambda_param=svm_lambda_param, kernel='poly', degree=svm_degree, n_averaged_states=N_AVERAGED_STATES)\n",
    "svm.fit(X_train, y_train)\n",
    "predictions = svm.predict(X_test)\n",
    "plot_metrics(predictions, y_test, 'k-SVM stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01d1fb2",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b7d910",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(n_iters=lr_n_iters, lambda_param=lr_lambda_param, learning_rate=lr_learning_rate, kernel='poly', degree=lr_degree)\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "plot_metrics(predictions, y_test, 'k-LR stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386ba630",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "With kernel methods, SVM performance are similar while logistic regression performance slightly improved. It is also notable that recall metrics are consistently higher than precision across both models. Regarding accuracy, performance approximates 75%, which represents a satisfactory result considering the baseline probability established by the dataset imbalance (60-40)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jx69buz9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{metrics_for_print[0]['accuracy']:.3f},{metrics_for_print[2]['accuracy']:.3f},{metrics_for_print[1]['accuracy']:.3f},{metrics_for_print[3]['accuracy']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
